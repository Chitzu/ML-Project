Training loss on iteration 0 = 2.2993080615997314
Training loss on iteration 100 = 2.207044789791107
Training loss on iteration 200 = 2.0026374435424805
Training loss on iteration 300 = 1.927994009256363
Training loss on iteration 400 = 1.8818299627304078
Training loss on iteration 500 = 1.811140775680542
Training loss on iteration 600 = 1.7480764627456664
Training loss on iteration 700 = 1.7590076041221618
Training loss on iteration 800 = 1.7299293327331542
Training loss on iteration 900 = 1.7359675520658493
Training loss on iteration 1000 = 1.6626470875740051
Training loss on iteration 1100 = 1.6900748133659362
Training loss on iteration 1200 = 1.6428495967388153
Training loss on iteration 1300 = 1.6411560547351838
Training loss on iteration 1400 = 1.633330556154251
Training loss on iteration 1500 = 1.645768341422081
Training loss on iteration 1600 = 1.6603241211175919
Training loss on iteration 1700 = 1.5577098780870438
Training loss on iteration 1800 = 1.6057648682594299
Training loss on iteration 1900 = 1.5855951714515686
Training loss on iteration 2000 = 1.5325975126028062
Training loss on iteration 2100 = 1.5578804814815521
Training loss on iteration 2200 = 1.4986954098939895
Training loss on iteration 2300 = 1.5244075876474381
Training loss on iteration 2400 = 1.4462922322750091
Training loss on iteration 2500 = 1.4544317650794982
Training loss on iteration 2600 = 1.4992188513278961
Training loss on iteration 2700 = 1.45665995657444
Training loss on iteration 2800 = 1.4959586018323898
Training loss on iteration 2900 = 1.4057408791780472
Training loss on iteration 3000 = 1.4312226557731629
Training loss on iteration 3100 = 1.4336087238788604
Training loss on iteration 3200 = 1.4700535869598388
Training loss on iteration 3300 = 1.5033515447378158
Training loss on iteration 3400 = 1.5072864496707916
Training loss on iteration 3500 = 1.4321180522441863
Training loss on iteration 3600 = 1.4537067526578904
Training loss on iteration 3700 = 1.4268842434883118
Training loss on iteration 3800 = 1.426479576230049
Training loss on iteration 3900 = 1.412102136015892
Training loss on iteration 4000 = 1.397498944401741
Training loss on iteration 4100 = 1.4043024706840515
Training loss on iteration 4200 = 1.3732588911056518
Training loss on iteration 4300 = 1.4325212848186493
Training loss on iteration 4400 = 1.4058740168809891
Training loss on iteration 4500 = 1.4320197749137877
Training loss on iteration 4600 = 1.3295184805989266
Training loss on iteration 4700 = 1.3687677294015885
Training loss on iteration 4800 = 1.351750412583351
Training loss on iteration 4900 = 1.3914228850603103
Training loss on iteration 0 = 0.6645140647888184
Training loss on iteration 100 = 1.3577493527531623
Training loss on iteration 200 = 1.3277271485328674
Training loss on iteration 300 = 1.3240434420108795
Training loss on iteration 400 = 1.3741645109653473
Training loss on iteration 500 = 1.3717067164182664
Training loss on iteration 600 = 1.3494643813371658
Training loss on iteration 700 = 1.273485370874405
Training loss on iteration 800 = 1.3047821733355522
Training loss on iteration 900 = 1.2917992228269577
Training loss on iteration 1000 = 1.2770544570684432
Training loss on iteration 1100 = 1.341563905477524
Training loss on iteration 1200 = 1.296145195364952
Training loss on iteration 1300 = 1.3200908225774766
Training loss on iteration 1400 = 1.2732780092954636
Training loss on iteration 1500 = 1.3044316399097442
Training loss on iteration 1600 = 1.3642063009738923
Training loss on iteration 1700 = 1.278831001520157
Training loss on iteration 1800 = 1.2630886656045914
Training loss on iteration 1900 = 1.2893394821882247
Training loss on iteration 2000 = 1.2883871257305146
Training loss on iteration 2100 = 1.27442218542099
Training loss on iteration 2200 = 1.260692257285118
Training loss on iteration 2300 = 1.2816914844512939
Training loss on iteration 2400 = 1.2253748521208763
Training loss on iteration 2500 = 1.2248544162511825
Training loss on iteration 2600 = 1.2424372345209123
Training loss on iteration 2700 = 1.2321560835838319
Training loss on iteration 2800 = 1.2345188868045807
Training loss on iteration 2900 = 1.1890905272960663
Training loss on iteration 3000 = 1.2158332407474517
Training loss on iteration 3100 = 1.202947832942009
Training loss on iteration 3200 = 1.264462460875511
Training loss on iteration 3300 = 1.2989445465803147
Training loss on iteration 3400 = 1.3048206984996795
Training loss on iteration 3500 = 1.224348263144493
Training loss on iteration 3600 = 1.265745947957039
Training loss on iteration 3700 = 1.2323452019691468
Training loss on iteration 3800 = 1.231137088239193
Training loss on iteration 3900 = 1.2318629258871079
Training loss on iteration 4000 = 1.2138375324010848
Training loss on iteration 4100 = 1.238306506872177
Training loss on iteration 4200 = 1.2030528211593627
Training loss on iteration 4300 = 1.230009297132492
Training loss on iteration 4400 = 1.2139516794681549
Training loss on iteration 4500 = 1.2608444267511367
Training loss on iteration 4600 = 1.1188185620307922
Training loss on iteration 4700 = 1.1766230070590973
Training loss on iteration 4800 = 1.2032086449861525
Training loss on iteration 4900 = 1.1974753630161286
Training loss on iteration 0 = 0.8802728652954102
Training loss on iteration 100 = 1.1984789636731148
Training loss on iteration 200 = 1.1714513570070266
Training loss on iteration 300 = 1.1337455093860627
Training loss on iteration 400 = 1.2166823545098304
Training loss on iteration 500 = 1.1863731762766838
Training loss on iteration 600 = 1.2131282922625541
Training loss on iteration 700 = 1.134044618010521
Training loss on iteration 800 = 1.168628535568714
Training loss on iteration 900 = 1.1570755332708358
Training loss on iteration 1000 = 1.1405659514665603
Training loss on iteration 1100 = 1.1978410983085632
Training loss on iteration 1200 = 1.1628346219658852
Training loss on iteration 1300 = 1.1938570594787599
Training loss on iteration 1400 = 1.163402874469757
Training loss on iteration 1500 = 1.1730064341425896
Training loss on iteration 1600 = 1.225770034790039
Training loss on iteration 1700 = 1.1583960556983948
Training loss on iteration 1800 = 1.1414123171567916
Training loss on iteration 1900 = 1.1915874356031417
Training loss on iteration 2000 = 1.1510300439596177
Training loss on iteration 2100 = 1.1572687020897865
Training loss on iteration 2200 = 1.1483891081809998
Training loss on iteration 2300 = 1.145780735015869
Training loss on iteration 2400 = 1.1039658406376838
Training loss on iteration 2500 = 1.103904567360878
Training loss on iteration 2600 = 1.1187991306185723
Training loss on iteration 2700 = 1.115608322620392
Training loss on iteration 2800 = 1.1306564030051232
Training loss on iteration 2900 = 1.0898667323589324
Training loss on iteration 3000 = 1.1178252699971198
Training loss on iteration 3100 = 1.109740605354309
Training loss on iteration 3200 = 1.1464872667193413
Training loss on iteration 3300 = 1.2163126522302627
Training loss on iteration 3400 = 1.2004258418083191
Training loss on iteration 3500 = 1.1482375526428223
Training loss on iteration 3600 = 1.1604609924554825
Training loss on iteration 3700 = 1.1332529473304749
Training loss on iteration 3800 = 1.1437917295098305
Training loss on iteration 3900 = 1.1348894447088242
Training loss on iteration 4000 = 1.1102847278118133
Training loss on iteration 4100 = 1.1402225801348687
Training loss on iteration 4200 = 1.085735940039158
Training loss on iteration 4300 = 1.1308345475792885
Training loss on iteration 4400 = 1.119570120871067
Training loss on iteration 4500 = 1.1676320219039917
Training loss on iteration 4600 = 0.9974978984892369
Training loss on iteration 4700 = 1.097840460538864
Training loss on iteration 4800 = 1.1342700535058976
Training loss on iteration 4900 = 1.090791490972042
Training loss on iteration 0 = 0.7818467020988464
Training loss on iteration 100 = 1.0889785093069078
Training loss on iteration 200 = 1.090877505838871
Training loss on iteration 300 = 1.0382330423593522
Training loss on iteration 400 = 1.137086708843708
Training loss on iteration 500 = 1.0927275043725968
Training loss on iteration 600 = 1.11743622392416
Training loss on iteration 700 = 1.083129893541336
Training loss on iteration 800 = 1.0857574859261512
Training loss on iteration 900 = 1.0895006546378136
Training loss on iteration 1000 = 1.0520833945274353
Training loss on iteration 1100 = 1.1123357418179511
Training loss on iteration 1200 = 1.0908089987933636
Training loss on iteration 1300 = 1.1105150079727173
Training loss on iteration 1400 = 1.0984593227505683
Training loss on iteration 1500 = 1.084687233567238
Training loss on iteration 1600 = 1.1501296198368072
Training loss on iteration 1700 = 1.0802096971869468
Training loss on iteration 1800 = 1.0612420430779457
Training loss on iteration 1900 = 1.0960163336992264
Training loss on iteration 2000 = 1.0754598438739777
Training loss on iteration 2100 = 1.0820650246739387
Training loss on iteration 2200 = 1.0705639189481735
Training loss on iteration 2300 = 1.074661644101143
Training loss on iteration 2400 = 1.0183821043372154
Training loss on iteration 2500 = 1.0446887382864951
Training loss on iteration 2600 = 1.0381624022126197
Training loss on iteration 2700 = 1.0304330360889435
Training loss on iteration 2800 = 1.0431506776809691
Training loss on iteration 2900 = 1.0463123896718025
Training loss on iteration 3000 = 1.0367626819014548
Training loss on iteration 3100 = 1.0556584411859513
Training loss on iteration 3200 = 1.091115260720253
Training loss on iteration 3300 = 1.1601475474238396
Training loss on iteration 3400 = 1.1605678004026414
Training loss on iteration 3500 = 1.0858770471811294
Training loss on iteration 3600 = 1.1007792496681212
Training loss on iteration 3700 = 1.065541675388813
Training loss on iteration 3800 = 1.0970300188660622
Training loss on iteration 3900 = 1.0648294460773469
Training loss on iteration 4000 = 1.0518555150926112
Training loss on iteration 4100 = 1.080062227845192
Training loss on iteration 4200 = 1.0149675983190536
Training loss on iteration 4300 = 1.062426640689373
Training loss on iteration 4400 = 1.0714657640457153
Training loss on iteration 4500 = 1.1066452053189277
Training loss on iteration 4600 = 0.9402465349435807
Training loss on iteration 4700 = 1.0454414981603621
Training loss on iteration 4800 = 1.0589610999822616
Training loss on iteration 4900 = 1.003349514901638
Training loss on iteration 0 = 2.2888569831848145
Training loss on iteration 100 = 2.239264761209488
Training loss on iteration 200 = 2.092633378505707
Training loss on iteration 300 = 1.997715722322464
Training loss on iteration 400 = 1.9450854992866515
Training loss on iteration 500 = 1.8595796537399292
Training loss on iteration 600 = 1.7747460651397704
Training loss on iteration 700 = 1.768372995853424
Training loss on iteration 800 = 1.7225672674179078
Training loss on iteration 900 = 1.6907454454898834
Training loss on iteration 1000 = 1.6473447918891906
Training loss on iteration 1100 = 1.6890278041362763
Training loss on iteration 1200 = 1.6174641048908234
Training loss on iteration 1300 = 1.6123785269260407
Training loss on iteration 1400 = 1.5890071946382522
Training loss on iteration 1500 = 1.5851698726415635
Training loss on iteration 1600 = 1.6451043665409089
Training loss on iteration 1700 = 1.520556310415268
Training loss on iteration 1800 = 1.5630411380529403
Training loss on iteration 1900 = 1.5301953601837157
Training loss on iteration 2000 = 1.525182281732559
Training loss on iteration 2100 = 1.510338837504387
Training loss on iteration 2200 = 1.4667206633090972
Training loss on iteration 2300 = 1.4834213465452195
Training loss on iteration 2400 = 1.4696681523323059
Training loss on iteration 2500 = 1.433960827589035
Training loss on iteration 2600 = 1.4542145758867264
Training loss on iteration 2700 = 1.4588421642780305
Training loss on iteration 2800 = 1.444859430193901
Training loss on iteration 2900 = 1.3936530429124832
Training loss on iteration 3000 = 1.4479485243558883
Training loss on iteration 3100 = 1.4252349120378494
Training loss on iteration 3200 = 1.43727127969265
Training loss on iteration 3300 = 1.5080894684791566
Training loss on iteration 3400 = 1.4886180406808853
Training loss on iteration 3500 = 1.4282309848070145
Training loss on iteration 3600 = 1.4581208008527755
Training loss on iteration 3700 = 1.4195549350976944
Training loss on iteration 3800 = 1.4117118138074876
Training loss on iteration 3900 = 1.4032567757368088
Training loss on iteration 4000 = 1.4065146780014037
Training loss on iteration 4100 = 1.3865804469585419
Training loss on iteration 4200 = 1.3793377178907393
Training loss on iteration 4300 = 1.4166718155145646
Training loss on iteration 4400 = 1.366424379348755
Training loss on iteration 4500 = 1.40906420648098
Training loss on iteration 4600 = 1.314784716963768
Training loss on iteration 4700 = 1.3757422250509261
Training loss on iteration 4800 = 1.3519150161743163
Training loss on iteration 4900 = 1.3953677558898925
Training loss on iteration 0 = 0.9050825238227844
Training loss on iteration 100 = 1.3453550019860268
Training loss on iteration 200 = 1.3607234328985214
Training loss on iteration 300 = 1.3156292963027953
